{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Index Type Comparison: Flat vs IVF\n",
    "\n",
    "This notebook compares the performance of **Flat** (brute force) and **IVF** (Inverted File) indexes.\n",
    "\n",
    "**Flat Index:**\n",
    "- Brute force search using cosine similarity\n",
    "- O(n) search complexity\n",
    "- 100% recall accuracy\n",
    "- Best for smaller datasets (<100k vectors)\n",
    "\n",
    "**IVF Index:**\n",
    "- Clusters vectors using k-means\n",
    "- Searches only relevant clusters (nprobe)\n",
    "- Faster search, but approximate results\n",
    "- Better for larger datasets\n",
    "\n",
    "**Setup:** Start the server with `make start` before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import time\n",
    "from client import StackAIClient\n",
    "\n",
    "client = StackAIClient()\n",
    "client.print_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Configuration\n\nChoose a preset or customize parameters:\n\n| Preset | Chunks | Time Estimate | Use Case |\n|--------|--------|---------------|----------|\n| `small` | 100 | ~30 sec | Quick sanity check |\n| `medium` | 500 | ~2 min | See some differences |\n| `large` | 2,000 | ~8 min | Clear IVF advantages |\n| `xlarge` | 5,000 | ~20 min | Stress test |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# === CHOOSE A PRESET ===\nPRESET = \"small\"  # Options: \"small\", \"medium\", \"large\", \"xlarge\", \"custom\"\n\n# Preset configurations\nPRESETS = {\n    \"small\":  {\"chunks\": 100,   \"batch\": 50,  \"queries\": 10},\n    \"medium\": {\"chunks\": 500,   \"batch\": 100, \"queries\": 20},\n    \"large\":  {\"chunks\": 2000,  \"batch\": 200, \"queries\": 30},\n    \"xlarge\": {\"chunks\": 5000,  \"batch\": 500, \"queries\": 50},\n    \"custom\": {\"chunks\": 100,   \"batch\": 50,  \"queries\": 10},  # Edit these\n}\n\n# Apply preset (or use custom values)\nconfig = PRESETS[PRESET]\nNUM_CHUNKS = config[\"chunks\"]\nBATCH_SIZE = config[\"batch\"]\nNUM_QUERIES = config[\"queries\"]\nK_VALUES = [5, 10, 20]\n\nprint(f\"Preset: {PRESET.upper()}\")\nprint(f\"  Chunks per index: {NUM_CHUNKS:,}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Search queries: {NUM_QUERIES}\")\nprint(f\"  K values: {K_VALUES}\")\n\n# Estimate time (rough: ~0.3s per chunk for embedding API call)\nest_minutes = (NUM_CHUNKS * 2 * 0.3) / 60\nprint(f\"\\nEstimated time: ~{est_minutes:.0f} minutes (mostly embedding API calls)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Generate sample text chunks for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts about various topics\n",
    "TOPICS = [\n",
    "    \"Machine learning algorithms process data to find patterns and make predictions.\",\n",
    "    \"Neural networks are inspired by biological brain structures and connections.\",\n",
    "    \"Deep learning uses multiple layers to extract hierarchical features from data.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Computer vision systems can identify objects and scenes in images.\",\n",
    "    \"Reinforcement learning trains agents through rewards and penalties.\",\n",
    "    \"Transformers use attention mechanisms for sequence processing tasks.\",\n",
    "    \"Vector databases store embeddings for fast similarity search.\",\n",
    "    \"Clustering algorithms group similar data points together.\",\n",
    "    \"Dimensionality reduction compresses high-dimensional data.\",\n",
    "    \"Gradient descent optimizes model parameters iteratively.\",\n",
    "    \"Convolutional networks excel at processing grid-like data.\",\n",
    "    \"Recurrent networks handle sequential data with memory.\",\n",
    "    \"Attention mechanisms help models focus on relevant information.\",\n",
    "    \"Transfer learning reuses knowledge from pre-trained models.\",\n",
    "    \"Data augmentation increases training set diversity.\",\n",
    "    \"Regularization techniques prevent model overfitting.\",\n",
    "    \"Batch normalization stabilizes neural network training.\",\n",
    "    \"Dropout randomly deactivates neurons during training.\",\n",
    "    \"Cross-validation estimates model generalization performance.\",\n",
    "]\n",
    "\n",
    "# Generate chunks by cycling through topics with variations\n",
    "def generate_chunks(n: int) -> list[dict]:\n",
    "    chunks = []\n",
    "    for i in range(n):\n",
    "        topic = TOPICS[i % len(TOPICS)]\n",
    "        text = f\"[Chunk {i+1}] {topic} This is variation {i // len(TOPICS) + 1}.\"\n",
    "        chunks.append({\"text\": text, \"metadata\": {\"index\": i}})\n",
    "    return chunks\n",
    "\n",
    "# Sample queries\n",
    "QUERIES = [\n",
    "    \"How does machine learning work?\",\n",
    "    \"What are neural networks?\",\n",
    "    \"Explain deep learning\",\n",
    "    \"How do transformers process text?\",\n",
    "    \"What is computer vision?\",\n",
    "    \"How does clustering work?\",\n",
    "    \"What is gradient descent?\",\n",
    "    \"Explain attention mechanisms\",\n",
    "    \"What is transfer learning?\",\n",
    "    \"How to prevent overfitting?\",\n",
    "]\n",
    "\n",
    "print(f\"Generated {len(TOPICS)} topic templates\")\n",
    "print(f\"Sample queries: {len(QUERIES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Create Test Libraries\n",
    "\n",
    "Create one library with Flat index and one with IVF index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create libraries with different index types\n",
    "flat_lib = client.create_library(\"Index Comparison - Flat\", index_type=\"flat\")\n",
    "ivf_lib = client.create_library(\"Index Comparison - IVF\", index_type=\"ivf\")\n",
    "\n",
    "print(f\"Created Flat library: ID={flat_lib['id']}, index_type={flat_lib['index_type']}\")\n",
    "print(f\"Created IVF library: ID={ivf_lib['id']}, index_type={ivf_lib['index_type']}\")\n",
    "\n",
    "# Create documents\n",
    "flat_doc = client.create_document(flat_lib['id'], \"Test Document\")\n",
    "ivf_doc = client.create_document(ivf_lib['id'], \"Test Document\")\n",
    "\n",
    "print(f\"Created documents: flat_doc={flat_doc['id']}, ivf_doc={ivf_doc['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Benchmark: Chunk Creation\n",
    "\n",
    "Compare the time to create and index chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = generate_chunks(NUM_CHUNKS)\n",
    "print(f\"Generated {len(chunks)} chunks\")\n",
    "\n",
    "# Time Flat index chunk creation\n",
    "print(\"\\n--- Flat Index: Creating chunks ---\")\n",
    "flat_start = time.perf_counter()\n",
    "for i in range(0, len(chunks), BATCH_SIZE):\n",
    "    batch = chunks[i:i + BATCH_SIZE]\n",
    "    client.create_chunks_batch(flat_doc['id'], batch)\n",
    "    print(f\"  Batch {i // BATCH_SIZE + 1}: {len(batch)} chunks\")\n",
    "flat_create_time = time.perf_counter() - flat_start\n",
    "print(f\"Flat index creation time: {flat_create_time:.2f}s\")\n",
    "\n",
    "# Time IVF index chunk creation\n",
    "print(\"\\n--- IVF Index: Creating chunks ---\")\n",
    "ivf_start = time.perf_counter()\n",
    "for i in range(0, len(chunks), BATCH_SIZE):\n",
    "    batch = chunks[i:i + BATCH_SIZE]\n",
    "    client.create_chunks_batch(ivf_doc['id'], batch)\n",
    "    print(f\"  Batch {i // BATCH_SIZE + 1}: {len(batch)} chunks\")\n",
    "ivf_create_time = time.perf_counter() - ivf_start\n",
    "print(f\"IVF index creation time: {ivf_create_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n=== Creation Summary ===\")\n",
    "print(f\"Flat: {flat_create_time:.2f}s ({NUM_CHUNKS / flat_create_time:.1f} chunks/sec)\")\n",
    "print(f\"IVF:  {ivf_create_time:.2f}s ({NUM_CHUNKS / ivf_create_time:.1f} chunks/sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Benchmark: Search Performance\n",
    "\n",
    "Compare search times across different k values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def benchmark_search(library_id: int, queries: list[str], k: int, num_runs: int = 1) -> list[float]:\n",
    "    \"\"\"Run searches and return timing results.\"\"\"\n",
    "    times = []\n",
    "    for query in queries[:num_runs]:\n",
    "        start = time.perf_counter()\n",
    "        client.search(library_id, query, k=k)\n",
    "        elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "        times.append(elapsed)\n",
    "    return times\n",
    "\n",
    "results = {\"flat\": {}, \"ivf\": {}}\n",
    "\n",
    "for k in K_VALUES:\n",
    "    print(f\"\\n--- Benchmarking k={k} ---\")\n",
    "    \n",
    "    flat_times = benchmark_search(flat_lib['id'], QUERIES, k, NUM_QUERIES)\n",
    "    ivf_times = benchmark_search(ivf_lib['id'], QUERIES, k, NUM_QUERIES)\n",
    "    \n",
    "    results[\"flat\"][k] = flat_times\n",
    "    results[\"ivf\"][k] = ivf_times\n",
    "    \n",
    "    print(f\"  Flat: avg={statistics.mean(flat_times):.1f}ms, min={min(flat_times):.1f}ms, max={max(flat_times):.1f}ms\")\n",
    "    print(f\"  IVF:  avg={statistics.mean(ivf_times):.1f}ms, min={min(ivf_times):.1f}ms, max={max(ivf_times):.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"INDEX COMPARISON RESULTS\")\nprint(\"=\" * 60)\nprint(f\"\\nPreset: {PRESET.upper()}\")\nprint(f\"Dataset: {NUM_CHUNKS:,} chunks per index\")\nprint(f\"Queries: {NUM_QUERIES} per k value\")\n\nprint(\"\\n--- Creation Time ---\")\nprint(f\"  Flat: {flat_create_time:.2f}s ({NUM_CHUNKS / flat_create_time:.1f} chunks/sec)\")\nprint(f\"  IVF:  {ivf_create_time:.2f}s ({NUM_CHUNKS / ivf_create_time:.1f} chunks/sec)\")\ndiff = abs(flat_create_time - ivf_create_time)\nfaster = \"Flat\" if flat_create_time < ivf_create_time else \"IVF\"\nprint(f\"  Winner: {faster} by {diff:.2f}s\")\n\nprint(\"\\n--- Search Time (avg ms) ---\")\nprint(f\"{'k':>5} | {'Flat':>10} | {'IVF':>10} | {'Speedup':>10}\")\nprint(\"-\" * 45)\nfor k in K_VALUES:\n    flat_avg = statistics.mean(results['flat'][k])\n    ivf_avg = statistics.mean(results['ivf'][k])\n    if ivf_avg > 0 and flat_avg > 0:\n        if ivf_avg < flat_avg:\n            speedup = f\"IVF {flat_avg/ivf_avg:.1f}x\"\n        else:\n            speedup = f\"Flat {ivf_avg/flat_avg:.1f}x\"\n    else:\n        speedup = \"-\"\n    print(f\"{k:>5} | {flat_avg:>10.1f} | {ivf_avg:>10.1f} | {speedup:>10}\")\n\nprint(\"\\n--- Interpretation ---\")\nif NUM_CHUNKS < 500:\n    print(\"- Small dataset: Flat may be faster (less overhead)\")\n    print(\"- Try 'medium' or 'large' preset to see IVF benefits\")\nelif NUM_CHUNKS < 2000:\n    print(\"- Medium dataset: IVF should start showing advantages\")\n    print(\"- Search speedup becomes more noticeable\")\nelse:\n    print(\"- Large dataset: IVF search should be significantly faster\")\n    print(\"- Trade-off: IVF has slightly lower recall accuracy\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Search Result Comparison\n",
    "\n",
    "Compare actual search results to check recall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare search results for a specific query\n",
    "test_query = \"How does machine learning work?\"\n",
    "k = 5\n",
    "\n",
    "print(f'Query: \"{test_query}\"')\n",
    "print(f\"k={k}\")\n",
    "\n",
    "flat_results = client.search(flat_lib['id'], test_query, k=k)\n",
    "ivf_results = client.search(ivf_lib['id'], test_query, k=k)\n",
    "\n",
    "print(\"\\n--- Flat Index Results ---\")\n",
    "for i, r in enumerate(flat_results['results'], 1):\n",
    "    print(f\"  {i}. [score={r['score']:.4f}] {r['chunk']['text'][:60]}...\")\n",
    "\n",
    "print(\"\\n--- IVF Index Results ---\")\n",
    "for i, r in enumerate(ivf_results['results'], 1):\n",
    "    print(f\"  {i}. [score={r['score']:.4f}] {r['chunk']['text'][:60]}...\")\n",
    "\n",
    "# Calculate overlap\n",
    "flat_ids = set(r['chunk']['id'] for r in flat_results['results'])\n",
    "ivf_ids = set(r['chunk']['id'] for r in ivf_results['results'])\n",
    "overlap = len(flat_ids & ivf_ids)\n",
    "print(f\"\\nResult overlap: {overlap}/{k} ({overlap/k*100:.0f}% recall)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the test libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_library(flat_lib['id'])\n",
    "client.delete_library(ivf_lib['id'])\n",
    "print(\"Test libraries deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}