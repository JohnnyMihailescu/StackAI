{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Index Type Comparison: Flat vs IVF\n",
    "\n",
    "This notebook compares the performance of **Flat** (brute force) and **IVF** (Inverted File) indexes.\n",
    "\n",
    "**Flat Index:**\n",
    "- Brute force search using cosine similarity\n",
    "- O(n) search complexity\n",
    "- 100% recall accuracy\n",
    "- Best for smaller datasets (<100k vectors)\n",
    "\n",
    "**IVF Index:**\n",
    "- Clusters vectors using k-means\n",
    "- Searches only relevant clusters (nprobe)\n",
    "- Faster search, but approximate results\n",
    "- Better for larger datasets\n",
    "\n",
    "**Setup:** Start the server with `make start` before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is running\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import time\n",
    "from client import StackAIClient\n",
    "\n",
    "client = StackAIClient()\n",
    "client.print_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Configuration\n\nChoose a preset or customize parameters:\n\n| Preset | Chunks | Est. Time (per index) | Use Case |\n|--------|--------|----------------------|----------|\n| `small` | 100 | ~1 sec | Quick sanity check |\n| `medium` | 500 | ~4 sec | See differences emerge |\n| `large` | 10,000 | ~80 sec | Clear IVF search advantage |\n| `xlarge` | 100,000 | ~13 min | Stress test |\n\n*Times based on ~128 chunks/sec with Cohere embedding API.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# === CHOOSE A PRESET ===\nPRESET = \"small\"  # Options: \"small\", \"medium\", \"large\", \"xlarge\", \"custom\"\n\n# Preset configurations\nPRESETS = {\n    \"small\":  {\"chunks\": 100,   \"batch\": 50,  \"queries\": 10},\n    \"medium\": {\"chunks\": 500,   \"batch\": 100, \"queries\": 20},\n    \"large\":  {\"chunks\": 10000,  \"batch\": 200, \"queries\": 30},\n    \"xlarge\": {\"chunks\": 100000,  \"batch\": 500, \"queries\": 50},\n    \"custom\": {\"chunks\": 100,   \"batch\": 50,  \"queries\": 10},  # Edit these\n}\n\n# Apply preset (or use custom values)\nconfig = PRESETS[PRESET]\nNUM_CHUNKS = config[\"chunks\"]\nBATCH_SIZE = config[\"batch\"]\nNUM_QUERIES = config[\"queries\"]\nK_VALUES = [5, 10, 20]\n\nprint(f\"Preset: {PRESET.upper()}\")\nprint(f\"  Chunks per index: {NUM_CHUNKS:,}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Search queries: {NUM_QUERIES}\")\nprint(f\"  K values: {K_VALUES}\")\n\n# Estimate time based on ~128 chunks/sec (Cohere batching at 96 texts/call)\nCHUNKS_PER_SEC = 128\nest_seconds = (NUM_CHUNKS * 2) / CHUNKS_PER_SEC  # x2 for both indexes\nif est_seconds < 60:\n    print(f\"\\nEstimated creation time: ~{est_seconds:.0f} seconds\")\nelse:\n    print(f\"\\nEstimated creation time: ~{est_seconds / 60:.1f} minutes\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Generate sample text chunks for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts about various topics\n",
    "TOPICS = [\n",
    "    \"Machine learning algorithms process data to find patterns and make predictions.\",\n",
    "    \"Neural networks are inspired by biological brain structures and connections.\",\n",
    "    \"Deep learning uses multiple layers to extract hierarchical features from data.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Computer vision systems can identify objects and scenes in images.\",\n",
    "    \"Reinforcement learning trains agents through rewards and penalties.\",\n",
    "    \"Transformers use attention mechanisms for sequence processing tasks.\",\n",
    "    \"Vector databases store embeddings for fast similarity search.\",\n",
    "    \"Clustering algorithms group similar data points together.\",\n",
    "    \"Dimensionality reduction compresses high-dimensional data.\",\n",
    "    \"Gradient descent optimizes model parameters iteratively.\",\n",
    "    \"Convolutional networks excel at processing grid-like data.\",\n",
    "    \"Recurrent networks handle sequential data with memory.\",\n",
    "    \"Attention mechanisms help models focus on relevant information.\",\n",
    "    \"Transfer learning reuses knowledge from pre-trained models.\",\n",
    "    \"Data augmentation increases training set diversity.\",\n",
    "    \"Regularization techniques prevent model overfitting.\",\n",
    "    \"Batch normalization stabilizes neural network training.\",\n",
    "    \"Dropout randomly deactivates neurons during training.\",\n",
    "    \"Cross-validation estimates model generalization performance.\",\n",
    "]\n",
    "\n",
    "# Generate chunks by cycling through topics with variations\n",
    "def generate_chunks(n: int) -> list[dict]:\n",
    "    chunks = []\n",
    "    for i in range(n):\n",
    "        topic = TOPICS[i % len(TOPICS)]\n",
    "        text = f\"[Chunk {i+1}] {topic} This is variation {i // len(TOPICS) + 1}.\"\n",
    "        chunks.append({\"text\": text, \"metadata\": {\"index\": i}})\n",
    "    return chunks\n",
    "\n",
    "# Sample queries\n",
    "QUERIES = [\n",
    "    \"How does machine learning work?\",\n",
    "    \"What are neural networks?\",\n",
    "    \"Explain deep learning\",\n",
    "    \"How do transformers process text?\",\n",
    "    \"What is computer vision?\",\n",
    "    \"How does clustering work?\",\n",
    "    \"What is gradient descent?\",\n",
    "    \"Explain attention mechanisms\",\n",
    "    \"What is transfer learning?\",\n",
    "    \"How to prevent overfitting?\",\n",
    "]\n",
    "\n",
    "print(f\"Generated {len(TOPICS)} topic templates\")\n",
    "print(f\"Sample queries: {len(QUERIES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Create Test Libraries\n",
    "\n",
    "Create one library with Flat index and one with IVF index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create libraries with different index types\n",
    "flat_lib = client.create_library(\"Index Comparison - Flat\", index_type=\"flat\")\n",
    "ivf_lib = client.create_library(\"Index Comparison - IVF\", index_type=\"ivf\")\n",
    "\n",
    "print(f\"Created Flat library: ID={flat_lib['id']}, index_type={flat_lib['index_type']}\")\n",
    "print(f\"Created IVF library: ID={ivf_lib['id']}, index_type={ivf_lib['index_type']}\")\n",
    "\n",
    "# Create documents\n",
    "flat_doc = client.create_document(flat_lib['id'], \"Test Document\")\n",
    "ivf_doc = client.create_document(ivf_lib['id'], \"Test Document\")\n",
    "\n",
    "print(f\"Created documents: flat_doc={flat_doc['id']}, ivf_doc={ivf_doc['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Benchmark: Chunk Creation\n",
    "\n",
    "Compare the time to create and index chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = generate_chunks(NUM_CHUNKS)\n",
    "print(f\"Generated {len(chunks)} chunks\")\n",
    "\n",
    "# Time Flat index chunk creation\n",
    "print(\"\\n--- Flat Index: Creating chunks ---\")\n",
    "flat_start = time.perf_counter()\n",
    "for i in range(0, len(chunks), BATCH_SIZE):\n",
    "    batch = chunks[i:i + BATCH_SIZE]\n",
    "    client.create_chunks_batch(flat_doc['id'], batch)\n",
    "    print(f\"  Batch {i // BATCH_SIZE + 1}: {len(batch)} chunks\")\n",
    "flat_create_time = time.perf_counter() - flat_start\n",
    "print(f\"Flat index creation time: {flat_create_time:.2f}s\")\n",
    "\n",
    "# Time IVF index chunk creation\n",
    "print(\"\\n--- IVF Index: Creating chunks ---\")\n",
    "ivf_start = time.perf_counter()\n",
    "for i in range(0, len(chunks), BATCH_SIZE):\n",
    "    batch = chunks[i:i + BATCH_SIZE]\n",
    "    client.create_chunks_batch(ivf_doc['id'], batch)\n",
    "    print(f\"  Batch {i // BATCH_SIZE + 1}: {len(batch)} chunks\")\n",
    "ivf_create_time = time.perf_counter() - ivf_start\n",
    "print(f\"IVF index creation time: {ivf_create_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n=== Creation Summary ===\")\n",
    "print(f\"Flat: {flat_create_time:.2f}s ({NUM_CHUNKS / flat_create_time:.1f} chunks/sec)\")\n",
    "print(f\"IVF:  {ivf_create_time:.2f}s ({NUM_CHUNKS / ivf_create_time:.1f} chunks/sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Benchmark: Search Performance\n",
    "\n",
    "Compare search times across different k values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def benchmark_search(library_id: int, queries: list[str], k: int, num_runs: int = 1) -> list[float]:\n",
    "    \"\"\"Run searches and return timing results.\"\"\"\n",
    "    times = []\n",
    "    for query in queries[:num_runs]:\n",
    "        start = time.perf_counter()\n",
    "        client.search(library_id, query, k=k)\n",
    "        elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "        times.append(elapsed)\n",
    "    return times\n",
    "\n",
    "results = {\"flat\": {}, \"ivf\": {}}\n",
    "\n",
    "for k in K_VALUES:\n",
    "    print(f\"\\n--- Benchmarking k={k} ---\")\n",
    "    \n",
    "    flat_times = benchmark_search(flat_lib['id'], QUERIES, k, NUM_QUERIES)\n",
    "    ivf_times = benchmark_search(ivf_lib['id'], QUERIES, k, NUM_QUERIES)\n",
    "    \n",
    "    results[\"flat\"][k] = flat_times\n",
    "    results[\"ivf\"][k] = ivf_times\n",
    "    \n",
    "    print(f\"  Flat: avg={statistics.mean(flat_times):.1f}ms, min={min(flat_times):.1f}ms, max={max(flat_times):.1f}ms\")\n",
    "    print(f\"  IVF:  avg={statistics.mean(ivf_times):.1f}ms, min={min(ivf_times):.1f}ms, max={max(ivf_times):.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INDEX COMPARISON RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPreset: {PRESET.upper()}\")\n",
    "print(f\"Dataset: {NUM_CHUNKS:,} chunks per index\")\n",
    "print(f\"Queries: {NUM_QUERIES} per k value\")\n",
    "\n",
    "print(\"\\n--- Creation Time ---\")\n",
    "print(f\"  Flat: {flat_create_time:.2f}s ({NUM_CHUNKS / flat_create_time:.1f} chunks/sec)\")\n",
    "print(f\"  IVF:  {ivf_create_time:.2f}s ({NUM_CHUNKS / ivf_create_time:.1f} chunks/sec)\")\n",
    "diff = abs(flat_create_time - ivf_create_time)\n",
    "faster = \"Flat\" if flat_create_time < ivf_create_time else \"IVF\"\n",
    "print(f\"  Winner: {faster} by {diff:.2f}s\")\n",
    "\n",
    "print(\"\\n--- Search Time (avg ms) ---\")\n",
    "print(f\"{'k':>5} | {'Flat':>10} | {'IVF':>10} | {'Speedup':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for k in K_VALUES:\n",
    "    flat_avg = statistics.mean(results['flat'][k])\n",
    "    ivf_avg = statistics.mean(results['ivf'][k])\n",
    "    if ivf_avg > 0 and flat_avg > 0:\n",
    "        if ivf_avg < flat_avg:\n",
    "            speedup = f\"IVF {flat_avg/ivf_avg:.1f}x\"\n",
    "        else:\n",
    "            speedup = f\"Flat {ivf_avg/flat_avg:.1f}x\"\n",
    "    else:\n",
    "        speedup = \"-\"\n",
    "    print(f\"{k:>5} | {flat_avg:>10.1f} | {ivf_avg:>10.1f} | {speedup:>10}\")\n",
    "\n",
    "print(\"\\n--- Interpretation ---\")\n",
    "if NUM_CHUNKS < 500:\n",
    "    print(\"- Small dataset: Flat may be faster (less overhead)\")\n",
    "    print(\"- Try 'medium' or 'large' preset to see IVF benefits\")\n",
    "elif NUM_CHUNKS < 2000:\n",
    "    print(\"- Medium dataset: IVF should start showing advantages\")\n",
    "    print(\"- Search speedup becomes more noticeable\")\n",
    "else:\n",
    "    print(\"- Large dataset: IVF search should be significantly faster\")\n",
    "    print(\"- Trade-off: IVF has slightly lower recall accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Search Result Comparison\n",
    "\n",
    "Compare actual search results to check recall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Compare search results for a specific query\ntest_query = \"How does machine learning work?\"\nk = 5\n\nprint(f'Query: \"{test_query}\"')\nprint(f\"k={k}\")\n\nflat_results = client.search(flat_lib['id'], test_query, k=k)\nivf_results = client.search(ivf_lib['id'], test_query, k=k)\n\nprint(\"\\n--- Flat Index Results ---\")\nfor i, r in enumerate(flat_results['results'], 1):\n    print(f\"  {i}. [score={r['score']:.4f}] {r['chunk']['text'][:60]}...\")\n\nprint(\"\\n--- IVF Index Results ---\")\nfor i, r in enumerate(ivf_results['results'], 1):\n    print(f\"  {i}. [score={r['score']:.4f}] {r['chunk']['text'][:60]}...\")\n\n# Calculate overlap by TEXT content (not ID, since they're in different libraries)\nflat_texts = set(r['chunk']['text'] for r in flat_results['results'])\nivf_texts = set(r['chunk']['text'] for r in ivf_results['results'])\noverlap = len(flat_texts & ivf_texts)\nprint(f\"\\nResult overlap: {overlap}/{k} chunks have identical text\")\nprint(f\"IVF recall vs Flat: {overlap/k*100:.0f}%\")\n\nif overlap < k:\n    print(f\"\\nNote: {k - overlap} results differ. This is expected with IVF -\")\n    print(\"it searches fewer clusters for speed, trading some accuracy.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the test libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_library(flat_lib['id'])\n",
    "client.delete_library(ivf_lib['id'])\n",
    "print(\"Test libraries deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StackAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}